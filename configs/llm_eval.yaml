# LLM Baseline Evaluation Configuration
# Follows MMAD paper's evaluation protocol

# Data paths (can be overridden by environment variables)
data:
  root: datasets/MMAD  # MMAD_DATA_ROOT
  mmad_json: datasets/MMAD/mmad.json  # MMAD_JSON_PATH

# LLM settings
llm:
  # Model selection: gpt-4o, gpt-4o-mini, claude, claude-sonnet, claude-haiku
  model: gpt-4o

  # API settings (use environment variables for keys)
  # OPENAI_API_KEY for GPT models
  # ANTHROPIC_API_KEY for Claude models

  max_tokens: 200
  max_retries: 5
  max_image_size: [512, 512]  # Resize images to this max size

# Evaluation settings
eval:
  # Few-shot settings (paper uses 1-shot by default)
  few_shot_k: 1

  # Template selection
  # true: use similar_templates (better for the task)
  # false: use random_templates
  use_similar_template: true

  # Batch mode: ask all questions in one API call
  # false: incremental mode (paper's approach - more accurate but slower)
  batch_mode: false

  # Maximum images to evaluate (null for all ~8366 images)
  max_images: null

  # Output directory
  output_dir: outputs/eval

# Resume from existing results
resume: true
